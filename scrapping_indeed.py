# -*- coding: utf-8 -*-
"""Scrapping_Indeed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dyq-t1L2hQP2gbWsWmClzOxmLUYslNQG
"""

# Import packages
import pandas as pd
import numpy as np
import requests
from scrapy.selector import Selector
from IPython.display import clear_output
import time
import re

# Iterating over search titles, which means that all words must be in the string for the job title
search_titles = ["Sustainability Manager", "Environmental Sustainability Specialist", "Corporate Social Responsibility (CSR) Coordinator", "Sustainable Development Analyst", "Renewable Energy Project Manager", "Climate Change Policy Analyst", "Circular Economy Strategist", "Green Building Consultant", "Sustainability Consultant", "Supply Chain Sustainability Manager"]

'''
Extract all job URLs for each search_title
'''

# Initialise empty list related to search term
job_URL_list = []
search_title_list = []

# Start timer
start = time.time()

# Iterating over search titles, job types and salary ranges..
for stitle in search_titles:

    # Crawl to the first page of the search results
    path = f"https://au.indeed.com/jobs?as_ttl={stitle.replace(' ','+')}&sr=directhire&radius=0&l=Australia&fromage=any&limit=50&sort=&psf=advsrch&from=advancedsearch"
    req = requests.get(path)

    # Extract the URLs for extra pages
    extra_page_URL_list = Selector(text=req.text).xpath('//div[@class="pagination"]/a/@href').getall()
    extra_page_URL_list = extra_page_URL_list[:-1] # drop the last URL, due to next button URL
    extra_page_URL_list = ['http://au.indeed.com' + i for i in extra_page_URL_list]

    print(f'Page 1 of {len(extra_page_URL_list)+1}:', end = '\n\n')

    print(f'Searching for all jobs titles that include: {stitle}',end = '\n')

    # Extract all URLs for all the job listings in page 1
    page_job_URL_list = Selector(text=req.text).xpath('//a[@data-tn-element="jobTitle"]/@href').getall()
    print(f"{len(page_job_URL_list)} job URLs extracted from Page 1", end = '\n\n')

    # Append all job URLs
    # Append search titles, job types and salary ranges to match number of job URLs
    job_URL_list += page_job_URL_list
    for i in range(len(page_job_URL_list)):
        search_title_list.append(stitle)


     # To make sure all list lengths are equal
    print('Job URL count:',len(job_URL_list))
    print('Search title count:',len(search_title_list))
    clear_output(wait=True)

    time.sleep(0.1)

    # Iterate through all the additional pages
    page_count = 1
    for extra_page in extra_page_URL_list:
        page_count += 1

        # Crawl to extra page
        req = requests.get(extra_page)
        print(f'Page {page_count} of {len(extra_page_URL_list)+1}:', end = '\n\n')

        print(f'Searching for all jobs titles that include: {stitle}',end = '\n')

        # Extract all URLs for all the job listings in extra pages
        page_job_URL_list = Selector(text=req.text).xpath('//a[@data-tn-element="jobTitle"]/@href').getall()
        print(f"{len(page_job_URL_list)} job URLs extracted from Page {page_count}", end = '\n\n')

        # Append all job URLs
        # Append search titles, job types and salary ranges to match number of job URLs
        job_URL_list += page_job_URL_list
        for i in range(len(page_job_URL_list)):
           search_title_list.append(stitle)

        # To make sure all list lengths are equal
        print('Job URL count:',len(job_URL_list))
        print('Search title count:',len(search_title_list))
        clear_output(wait=True)

        time.sleep(0.1)

job_URL_list = ['http://au.indeed.com' + i for i in job_URL_list]
print(f"Job URL scraping completed: {len(job_URL_list)}", end = '\n')

end = time.time()

"""#Part Two"""

import csv
from datetime import datetime
import requests
from bs4 import BeautifulSoup

#Setup the query and url
#template= "https://www.indeed.com/jobs?q=sustainability&l=Beloit%2C+WI&from=searchOnHP" -- initial url

def get_url(position, location):
    """Generate url from position and location"""
    template = 'https://www.indeed.com/jobs?q={}&l={}'
    position = position.replace(' ', '+')
    location = location.replace(' ', '+')
    url = template.format(position, location)
    return url

job_positions = ["Sustainability Manager", "Environmental Sustainability Specialist", "Corporate Social Responsibility (CSR) Coordinator", "Sustainable Development Analyst", "Renewable Energy Project Manager", "Climate Change Policy Analyst", "Circular Economy Strategist", "Green Building Consultant", "Sustainability Consultant", "Supply Chain Sustainability Manager"]

location = 'beloit wi'
for position in job_positions:
    url = get_url(position, location)
    print(url)

#Extract the html data
#tried to proxies, or headers to avoid this
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'
}

response = requests.get(url, headers=headers)

response

response.reason

#soup object to navigate the html text editor- I look for is html tag that encloses the highlighted role
soup = BeautifulSoup(response.text, 'html.parser')

cards = soup.find_all('div', 'jcs-JobTitle css-jspxzf eu4oa1w0')

"""<a id="job_0cdb55029eb47b25" data-mobtk="1hpoon26gk6dd811" data-jk="0cdb55029eb47b25" data-hiring-event="false" data-hide-spinner="true" role="button" aria-label="full details of Sustainability Operations Lead" class="jcs-JobTitle css-jspxzf eu4oa1w0" href="/rc/clk?jk=0cdb55029eb47b25&amp;bb=83SAZ_n1rThbM8SMKdR_lZzN2FhjTtYITPKdga7vOwWPWjOzUFza5hfUKx5W0XlwFvsuD36Bp9ufJxRUmGuniUfeNGEKvS9gzCgoXyE98m80SA7rbIvi0w%3D%3D&amp;xkcb=SoBD67M3D45TkXzdZB0LbzkdCdPP&amp;fccid=4c2ad73ae75d8284&amp;vjs=3"><span title="Sustainability Operations Lead" id="jobTitle-0cdb55029eb47b25">Sustainability Operations Lead</span></a>"""

#Prototype the model with a single record
card = cards[0]

job_title = card.h2.a.get('title')

company = card.find('span', 'company').text.strip()

job_location = card.find('div', 'recJobLoc').get('data-rc-loc')

post_date = card.find('span', 'date').text
today = datetime.today().strftime('%Y-%m-%d')

summary = card.find('div', 'summary').text.strip().replace('\n', ' ')

# this does not exists for all jobs, so handle the exceptions
salary_tag = card.find('span', 'salaryText')
if salary_tag:
    salary = salary_tag.text.strip()
else:
    salary = ''

job_url = 'https://www.indeed.com' + card.h2.a.get('href')

record = (job_title, company, job_location, post_date, today, summary, salary, job_url)

record

#Generalize the model with a function
def get_record(card):
    """Extract job data from a single record"""

    job_title = card.h2.a.get('title')
    company = card.find('span', 'company').text.strip()
    job_location = card.find('div', 'recJobLoc').get('data-rc-loc')
    post_date = card.find('span', 'date').text
    today = datetime.today().strftime('%Y-%m-%d')
    summary = card.find('div', 'summary').text.strip().replace('\n', ' ')
    job_url = 'https://www.indeed.com' + card.h2.a.get('href')

    # this does not exists for all jobs, so handle the exceptions
    salary_tag = card.find('span', 'salaryText')
    if salary_tag:
        salary = salary_tag.text.strip()
    else:
        salary = ''

    record = (job_title, company, job_location, post_date, today, summary, salary, job_url)
    return record

records = []

for card in cards:
    record = get_record(card)
    records.append(record)

#Get the next page
while True:
    try:
        url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')
    except AttributeError:
        break

    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    cards = soup.find_all('div', 'jobsearch-SerpJobCard')

    for card in cards:
        record = get_record(card)
        records.append(record)

#Putting it all together
import csv
from datetime import datetime
import requests
from bs4 import BeautifulSoup


def get_url(position, location):
    """Generate url from position and location"""
    template = 'https://www.indeed.com/jobs?q={}&l={}'
    position = position.replace(' ', '+')
    location = location.replace(' ', '+')
    url = template.format(position, location)
    return url


def get_record(card):
    """Extract job data from a single record"""

    job_title = card.h2.a.get('title')
    company = card.find('span', 'company').text.strip()
    job_location = card.find('div', 'recJobLoc').get('data-rc-loc')
    post_date = card.find('span', 'date').text
    today = datetime.today().strftime('%Y-%m-%d')
    summary = card.find('div', 'summary').text.strip().replace('\n', ' ')
    job_url = 'https://www.indeed.com' + card.h2.a.get('href')

    # this does not exists for all jobs, so handle the exceptions
    salary_tag = card.find('span', 'salaryText')
    if salary_tag:
        salary = salary_tag.text.strip()
    else:
        salary = ''

    record = (job_title, company, job_location, post_date, today, summary, salary, job_url)
    return record


def main(position, location):
    """Run the main program routine"""
    records = []
    url = get_url(position, location)

    # extract the job data
    while True:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        cards = soup.find_all('div', 'jobsearch-SerpJobCard')
        for card in cards:
            record = get_record(card)
            records.append(record)
        try:
            url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')
        except AttributeError:
            break

    # save the job data
    with open('results.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['JobTitle', 'Company', 'Location', 'PostDate', 'ExtractDate', 'Summary', 'Salary', 'JobUrl'])
        writer.writerows(records)

# run the main program
main('senior accountant', 'charlotte nc')

"""# Part Three"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup as bs
import mysql.connector
import time

# Connect to MySQL database
'''mydb = mysql.connector.connect(
    host="database-1.cux1s0fa60hj.us-east-2.rds.amazonaws.com",
    user="admin",
    password="Lenovo2002",
    database="database-1"
)'''

# Create cursor
cursor = mydb.cursor()

# Create the table if it doesn't exist
cursor.execute('''
    CREATE TABLE IF NOT EXISTS Marketing_Jobs_Indeed (
        JobId VARCHAR(20) PRIMARY KEY,
        Title VARCHAR(200),
        Company VARCHAR(100),
        Location VARCHAR(100),
        Apply VARCHAR(300),
        JobDescription TEXT,
        Salary VARCHAR(100)
    )
''')

# Commit the transaction and close the connection
mydb.commit()
cursor.close()
mydb.close()

# Define chrome_options
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument("executable_path=./chromedriver.exe")

wd = webdriver.Chrome(options = chrome_options)
time.sleep(1)

driver = webdriver.Chrome(options = chrome_options)
time.sleep(1)

def indeed_scraper(webpage, page_number):
    next_page = webpage + str(page_number)
    print(str(next_page))
    wd.get(next_page)

    jobs_src = wd.page_source
    soup = bs(jobs_src, "html.parser")
    jobs = soup.find_all("div", class_ = "job_seen_beacon")

    for job in jobs:
        job_head_elem = job.find("h2", class_ = "jobTitle")
        #JobId
        job_id = job_head_elem.find("span")["id"].split("-")[1]

        #Check if the JobId already exists in the table
        select_query = "SELECT JobId FROM marketing_jobs WHERE JobId = %s"
        cursor.execute(select_query, (job_id,))
        result = cursor.fetchone()

        if result is None:
            #Job Title
            job_title = job_head_elem.find("span").text.strip()

            #Job Link
            job_link = "https://www.indeed.com" + job_head_elem.find("a")["href"]

            #Company Name
            company_name = job.find("span", class_ = "companyName").text.strip()

            #Location
            location = job.find("div", class_ = "companyLocation").text.strip()

            driver.get(job_link)
            time.sleep(3)
            job_src = driver.page_source
            job_soup = bs(job_src, "html.parser")

            #Salary
            salary_elem = None
            for elem in job_soup.find_all("div", class_ = "css-tvvxwd ecydgvn1"):
                text = elem.text.strip()
                if any(char.isdigit() for char in text):
                    salary_elem = elem
                    break
            salary = salary_elem.text.strip() if salary_elem else ""

            #Job Description
            job_des_elem = job_soup.find("div", id = "jobDescriptionText")
            job_des = job_des_elem.text.strip() if job_des_elem else ""

            #Insert the data into the MySQL table
            insert_query = '''
                INSERT INTO marketing_jobs (JobId, Title, Company, Location, Apply, Salary, JobDescription)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            '''
            insert_values = (job_id, job_title, company_name, location, job_link, salary, job_des)
            cursor.execute(insert_query, insert_values)
            mydb.commit()
        else:
            print("Skipping duplicate JobId:", job_id)
    print("Data updated")

    if page_number < 1000:
        page_number += 10
        indeed_scraper(webpage, page_number)
    else:
        cursor.close()
        mydb.close()
        driver.quit
        wd.quit

indeed_scraper('https://www.indeed.com/jobs?q=marketing&l=United+States&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&radius=35&start=', 0)