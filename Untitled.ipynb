{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dd53b3a-f6da-44b5-a261-5d5ae1d2f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9d6b0c7-a7bb-43e3-9224-f2e45446778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_jobs(job):\n",
    "\n",
    "    username = driver.find_element(By.ID, \"session_key\")\n",
    "    password = driver.find_element(By.ID, \"session_password\")\n",
    "    username.send_keys('princenoworkhere@gmail.com')\n",
    "    password.send_keys('DataClinic')\n",
    "    login_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "    login_button.click()\n",
    "\n",
    "    driver.get(\"https://www.linkedin.com/jobs\")\n",
    "    search_box = driver.find_element(By.CLASS_NAME, \"jobs-search-box__text-input\")\n",
    "    search_box.send_keys(job)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Fetch jobs from the current page\n",
    "    all_jobs = get_current_page_jobs(driver)\n",
    "    return all_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa82ad91-6704-41b0-9182-cbed55f142d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_page_jobs(driver):\n",
    "    current_page_url = driver.current_url\n",
    "    c=1\n",
    "    while True:\n",
    "        \n",
    "        number_of_elements_found = 0\n",
    "        while True:\n",
    "            els = driver.find_elements(By.CSS_SELECTOR, '.job-card-list__insight')\n",
    "            if number_of_elements_found == len(els):\n",
    "                # Reached the end of loadable elements\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", els[-1])\n",
    "                time.sleep(2)\n",
    "                number_of_elements_found = len(els)\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                # Possible to get a StaleElementReferenceException. Ignore it and retry.\n",
    "                pass\n",
    "        jobs_links=[]\n",
    "        jobs = driver.find_elements(By.XPATH, \"//div[@class='full-width artdeco-entity-lockup__title ember-view']/a[@href]\")\n",
    "        for l in jobs:\n",
    "            jobs_links.append(l.get_attribute('href'))\n",
    "        get_each_job_info(jobs_links) \n",
    "        print('*'*100)\n",
    "        print(f'page {c} completed')\n",
    "        \n",
    "        driver.get(current_page_url)\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            c +=1\n",
    "            des='Page '+str(c)\n",
    "            dest=f\"//button[@aria-label='{des}']\"\n",
    "            next_page_button=driver.find_element_by_xpath(dest)\n",
    "            next_page_button.click()\n",
    "            time.sleep(2)\n",
    "            current_page_url=driver.current_url\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d2a6a45-444f-4577-b57d-f4248f1e9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_each_job_info(jobs, driver):\n",
    "    all_jobs = []\n",
    "    for j in jobs:\n",
    "        driver.get(j)\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "\n",
    "        # Extracting data\n",
    "        title = soup.find('h1', class_='t-24 t-bold jobs-unified-top-card__job-title').text if soup.find('h1', class_='t-24 t-bold jobs-unified-top-card__job-title') else \"\"\n",
    "        company_name = soup.find('span', class_='jobs-unified-top-card__company-name').text.strip() if soup.find('span', class_='jobs-unified-top-card__company-name') else \"\"\n",
    "        location = soup.find('span', class_='jobs-unified-top-card__bullet').text.strip() if soup.find('span', class_='jobs-unified-top-card__bullet') else \"\"\n",
    "        work_type = soup.find('span', class_='jobs-unified-top-card__workplace-type').text.strip() if soup.find('span', class_='jobs-unified-top-card__workplace-type') else \"\"\n",
    "        job_posted_date = 'No longer Available'\n",
    "        if soup.find('span', class_='jobs-unified-top-card__posted-date'):\n",
    "            d = soup.find('span', class_='jobs-unified-top-card__posted-date').text.strip()\n",
    "            if re.search(r'\\d', d):\n",
    "                days_ago = int(re.search(r'\\d', d).group())\n",
    "                job_posted_date = date.today() - timedelta(days_ago)\n",
    "\n",
    "        pay_range = \"unavailable\"\n",
    "        try:\n",
    "            pay_range = driver.find_element_by_xpath(\"//p[@class='t-16']\").text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        job = {\n",
    "            'job_posted_date': job_posted_date,\n",
    "            'title': title,\n",
    "            'location': location,\n",
    "            'company name': company_name,\n",
    "            'work type': work_type,\n",
    "            'scraping date': date.today(),\n",
    "            'pay range': pay_range,\n",
    "            'job_link': j\n",
    "        }\n",
    "        all_jobs.append(job)\n",
    "        print(\"job link\", j)\n",
    "\n",
    "    return all_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f271ebdc-17a2-4c01-b53d-e80e549b0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.linkedin.com'\n",
    "job='Data Scientist'\n",
    "location='egypt'\n",
    "all_jobs=[]\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "all_jobs=fetch_all_jobs(job)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
